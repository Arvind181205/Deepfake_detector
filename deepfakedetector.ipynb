{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q-kg3ZfjX3uK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import urllib3\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models  # type: ignore\n",
        "from tensorflow.keras.layers import LeakyReLU  # type: ignore\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint  # type: ignore\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetHandler:\n",
        "    \"\"\"\n",
        "    A class to handle dataset downloading, unzipping, loading, and processing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset_url, dataset_download_dir, dataset_file, dataset_dir, train_dir, test_dir, val_dir):\n",
        "        \"\"\"\n",
        "        Initialize the DatasetHandler with the specified parameters.\n",
        "\n",
        "        Args:\n",
        "            dataset_url (str): URL to download the dataset from.\n",
        "            dataset_download_dir (str): Directory to download the dataset to.\n",
        "            dataset_file (str): Name of the dataset file.\n",
        "            dataset_dir (str): Directory containing the dataset.\n",
        "            train_dir (str): Directory containing the training data.\n",
        "            test_dir (str): Directory containing the test data.\n",
        "            val_dir (str): Directory containing the validation data.\n",
        "        \"\"\"\n",
        "        self.dataset_url = dataset_url\n",
        "        self.dataset_download_dir = dataset_download_dir\n",
        "        self.dataset_file = dataset_file\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.train_dir = train_dir\n",
        "        self.test_dir = test_dir\n",
        "        self.val_dir = val_dir\n",
        "\n",
        "    def download_dataset(self):\n",
        "        \"\"\"\n",
        "        Download the dataset from the specified URL.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the dataset was successfully downloaded, False otherwise.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.dataset_download_dir):\n",
        "            os.makedirs(self.dataset_download_dir)\n",
        "        file_path = os.path.join(self.dataset_download_dir, self.dataset_file)\n",
        "        if os.path.exists(file_path):\n",
        "            print(f'dataset file {self.dataset_file} already exists at {file_path}')\n",
        "            return True\n",
        "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "        response = requests.get(self.dataset_url, stream=True, verify=False)\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        with open(file_path, 'wb') as file, tqdm(desc=self.dataset_file, total=total_size, unit='iB', unit_scale=True, unit_divisor=1024) as bar:\n",
        "            for data in response.iter_content(chunk_size=1024):\n",
        "                size = file.write(data)\n",
        "                bar.update(size)\n",
        "        print(f'dataset downloaded and saved to {file_path}')\n",
        "        return True\n",
        "\n",
        "    def unzip_dataset(self):\n",
        "        \"\"\"\n",
        "        Unzip the downloaded dataset file.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the dataset was successfully unzipped, False otherwise.\n",
        "        \"\"\"\n",
        "        file_path = os.path.join(self.dataset_download_dir, self.dataset_file)\n",
        "        if os.path.exists(self.dataset_dir):\n",
        "            print(f'dataset is already downloaded and extracted at {self.dataset_dir}')\n",
        "            return True\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f'dataset file {file_path} not found after download')\n",
        "            return False\n",
        "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(self.dataset_download_dir)\n",
        "        print(f'dataset extracted to {self.dataset_dir}')\n",
        "        return True\n",
        "\n",
        "    def get_image_dataset_from_directory(self, dir_name):\n",
        "        \"\"\"\n",
        "        Load image dataset from the specified directory.\n",
        "\n",
        "        Args:\n",
        "            dir_name (str): Name of the directory containing the dataset.\n",
        "\n",
        "        Returns:\n",
        "            tf.data.Dataset: Loaded image dataset.\n",
        "        \"\"\"\n",
        "        dir_path = os.path.join(self.dataset_dir, dir_name)\n",
        "        return tf.keras.utils.image_dataset_from_directory(\n",
        "            dir_path,\n",
        "            labels='inferred',\n",
        "            color_mode='rgb',\n",
        "            seed=42,\n",
        "            batch_size=64,\n",
        "            image_size=(128, 128)\n",
        "        )\n",
        "\n",
        "    def load_split_data(self):\n",
        "        \"\"\"\n",
        "        Load and split the dataset into training, validation, and test datasets.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Training, validation, and test datasets.\n",
        "        \"\"\"\n",
        "        train_data = self.get_image_dataset_from_directory(self.train_dir)\n",
        "        test_data = self.get_image_dataset_from_directory(self.test_dir)\n",
        "        val_data = self.get_image_dataset_from_directory(self.val_dir)\n",
        "        return train_data, test_data, val_data\n"
      ],
      "metadata": {
        "id": "XF3DvL-wYL-6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepfakeDetectorModel:\n",
        "    \"\"\"\n",
        "    A class to create and train a deepfake detection model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the DeepfakeDetectorModel by building the model.\n",
        "        \"\"\"\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"\n",
        "        Build the deepfake detection model architecture.\n",
        "\n",
        "        Returns:\n",
        "            tf.keras.Model: Built model.\n",
        "        \"\"\"\n",
        "        model = models.Sequential()\n",
        "        model.add(layers.Input(shape=(128, 128, 3)))\n",
        "        model.add(layers.Rescaling(1./127, name='rescaling'))\n",
        "        model.add(layers.Conv2D(32, (3, 3), strides=1, padding='same', activation='relu'))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "        model.add(layers.Conv2D(64, (3, 3), strides=1, padding='same', activation='relu'))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "        model.add(layers.Conv2D(128, (3, 3), strides=1, padding='same', activation='relu'))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "        model.add(layers.Conv2D(256, (3, 3), strides=1, padding='same', activation='relu'))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "        model.add(layers.Flatten())\n",
        "        model.add(layers.Dense(512, activation='relu'))\n",
        "        model.add(layers.Dropout(0.5))\n",
        "        model.add(layers.Dense(256, activation='relu'))\n",
        "        model.add(layers.Dropout(0.5))\n",
        "        model.add(layers.Dense(128, activation='relu'))\n",
        "        model.add(layers.Dense(1, activation='sigmoid'))\n",
        "        return model\n",
        "\n",
        "    def compile_model(self, learning_rate):\n",
        "        \"\"\"\n",
        "        Compile the deepfake detection model.\n",
        "\n",
        "        Args:\n",
        "            learning_rate (float): Learning rate for the optimizer.\n",
        "        \"\"\"\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "        self.model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
        "        )\n",
        "\n",
        "    def train_model(self, train_data, val_data, epochs):\n",
        "        \"\"\"\n",
        "        Train the deepfake detection model.\n",
        "\n",
        "        Args:\n",
        "            train_data (tf.data.Dataset): Training dataset.\n",
        "            val_data (tf.data.Dataset): Validation dataset.\n",
        "            epochs (int): Number of epochs to train the model.\n",
        "\n",
        "        Returns:\n",
        "            tf.keras.callbacks.History: History object containing training details.\n",
        "        \"\"\"\n",
        "        early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1)\n",
        "        model_checkpoint_callback = ModelCheckpoint('deepfake_detector_model_best.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "        return self.model.fit(\n",
        "            train_data,\n",
        "            validation_data=val_data,\n",
        "            epochs=epochs,\n",
        "            callbacks=[early_stopping_callback, reduce_lr_callback, model_checkpoint_callback]\n",
        "        )\n",
        "\n",
        "    def evaluate_model(self, test_data):\n",
        "        \"\"\"\n",
        "        Evaluate the deepfake detection model.\n",
        "\n",
        "        Args:\n",
        "            test_data (tf.data.Dataset): Test dataset.\n",
        "\n",
        "        Returns:\n",
        "            list: Evaluation metrics.\n",
        "        \"\"\"\n",
        "        return self.model.evaluate(test_data)\n",
        "\n",
        "    def save_model(self, path):\n",
        "        \"\"\"\n",
        "        Save the deepfake detection model to the specified path.\n",
        "\n",
        "        Args:\n",
        "            path (str): Path to save the model.\n",
        "        \"\"\"\n",
        "        self.model.save(path)\n"
      ],
      "metadata": {
        "id": "vfO3K8x5YTik"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainModel:\n",
        "    \"\"\"\n",
        "    A class to manage training of a deepfake detection model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset_url, dataset_download_dir, dataset_file, dataset_dir, train_dir, test_dir, val_dir):\n",
        "        \"\"\"\n",
        "        Initialize the TrainModel class with the specified parameters.\n",
        "\n",
        "        Args:\n",
        "            dataset_url (str): URL to download the dataset from.\n",
        "            dataset_download_dir (str): Directory to download the dataset to.\n",
        "            dataset_file (str): Name of the dataset file.\n",
        "            dataset_dir (str): Directory containing the dataset.\n",
        "            train_dir (str): Directory containing the training data.\n",
        "            test_dir (str): Directory containing the test data.\n",
        "            val_dir (str): Directory containing the validation data.\n",
        "        \"\"\"\n",
        "        self.dataset_handler = DatasetHandler(dataset_url, dataset_download_dir, dataset_file, dataset_dir, train_dir, test_dir, val_dir)\n",
        "\n",
        "    def run_training(self, learning_rate=0.0001, epochs=10):\n",
        "        \"\"\"\n",
        "        Run the training process for the deepfake detection model.\n",
        "\n",
        "        Args:\n",
        "            learning_rate (float): Learning rate for the optimizer.\n",
        "            epochs (int): Number of epochs to train the model.\n",
        "\n",
        "        Returns:\n",
        "            tuple: History object and evaluation metrics.\n",
        "        \"\"\"\n",
        "        if not self.dataset_handler.download_dataset():\n",
        "            print('failed to download dataset')\n",
        "            return\n",
        "        if not self.dataset_handler.unzip_dataset():\n",
        "            print('failed to unzip dataset')\n",
        "            return\n",
        "        train_data, test_data, val_data = self.dataset_handler.load_split_data()\n",
        "        model = DeepfakeDetectorModel()\n",
        "        model.compile_model(learning_rate)\n",
        "        history = model.train_model(train_data, val_data, epochs)\n",
        "        evaluation_metrics = model.evaluate_model(test_data)\n",
        "        model.save_model('deepfake_detector_model.keras')\n",
        "        return history, evaluation_metrics\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # config\n",
        "    dataset_url = 'https://www.kaggle.com/api/v1/datasets/download/manjilkarki/deepfake-and-real-images?datasetVersionNumber=1'\n",
        "    dataset_download_dir = './data'\n",
        "    dataset_file = 'dataset.zip'\n",
        "    dataset_dir = './data/Dataset'\n",
        "    train_dir = 'Train'\n",
        "    test_dir = 'Test'\n",
        "    val_dir = 'Validation'\n",
        "\n",
        "    # instantiate the TrainModel class with the specified configuration\n",
        "    trainer = TrainModel(\n",
        "        dataset_url=dataset_url,\n",
        "        dataset_download_dir=dataset_download_dir,\n",
        "        dataset_file=dataset_file,\n",
        "        dataset_dir=dataset_dir,\n",
        "        train_dir=train_dir,\n",
        "        test_dir=test_dir,\n",
        "        val_dir=val_dir\n",
        "    )\n",
        "\n",
        "    # train\n",
        "    history, evaluation_metrics = trainer.run_training(learning_rate=0.0001, epochs=10)\n",
        "\n",
        "    # metrics\n",
        "    print('evaluation metrics:', evaluation_metrics)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l6C6QVhYag8",
        "outputId": "b7c040e9-829e-411b-c176-29e30795f143"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset file dataset.zip already exists at ./data/dataset.zip\n",
            "dataset is already downloaded and extracted at ./data/Dataset\n",
            "Found 140002 files belonging to 2 classes.\n",
            "Found 10905 files belonging to 2 classes.\n",
            "Found 39428 files belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7227 - loss: 0.5573 - precision: 0.7156 - recall: 0.7358\n",
            "Epoch 1: val_loss improved from inf to 0.25320, saving model to deepfake_detector_model_best.keras\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 63ms/step - accuracy: 0.7227 - loss: 0.5572 - precision: 0.7156 - recall: 0.7358 - val_accuracy: 0.8903 - val_loss: 0.2532 - val_precision: 0.9101 - val_recall: 0.8670 - learning_rate: 1.0000e-04\n",
            "Epoch 2/10\n",
            "\u001b[1m2187/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9338 - loss: 0.1651 - precision: 0.9309 - recall: 0.9370\n",
            "Epoch 2: val_loss improved from 0.25320 to 0.17719, saving model to deepfake_detector_model_best.keras\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 59ms/step - accuracy: 0.9338 - loss: 0.1650 - precision: 0.9310 - recall: 0.9370 - val_accuracy: 0.9257 - val_loss: 0.1772 - val_precision: 0.9009 - val_recall: 0.9571 - learning_rate: 1.0000e-04\n",
            "Epoch 3/10\n",
            "\u001b[1m2187/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9572 - loss: 0.1085 - precision: 0.9542 - recall: 0.9603\n",
            "Epoch 3: val_loss did not improve from 0.17719\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 54ms/step - accuracy: 0.9572 - loss: 0.1085 - precision: 0.9542 - recall: 0.9603 - val_accuracy: 0.9284 - val_loss: 0.1817 - val_precision: 0.8891 - val_recall: 0.9796 - learning_rate: 1.0000e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m2187/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9692 - loss: 0.0796 - precision: 0.9667 - recall: 0.9719\n",
            "Epoch 4: val_loss improved from 0.17719 to 0.14591, saving model to deepfake_detector_model_best.keras\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 55ms/step - accuracy: 0.9692 - loss: 0.0796 - precision: 0.9667 - recall: 0.9719 - val_accuracy: 0.9437 - val_loss: 0.1459 - val_precision: 0.9516 - val_recall: 0.9354 - learning_rate: 1.0000e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m2187/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9751 - loss: 0.0628 - precision: 0.9728 - recall: 0.9775\n",
            "Epoch 5: val_loss did not improve from 0.14591\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 54ms/step - accuracy: 0.9751 - loss: 0.0628 - precision: 0.9728 - recall: 0.9775 - val_accuracy: 0.9128 - val_loss: 0.2331 - val_precision: 0.8641 - val_recall: 0.9803 - learning_rate: 1.0000e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9795 - loss: 0.0507 - precision: 0.9776 - recall: 0.9815\n",
            "Epoch 6: val_loss did not improve from 0.14591\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 55ms/step - accuracy: 0.9795 - loss: 0.0507 - precision: 0.9776 - recall: 0.9815 - val_accuracy: 0.9482 - val_loss: 0.1598 - val_precision: 0.9249 - val_recall: 0.9761 - learning_rate: 1.0000e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m2187/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9827 - loss: 0.0445 - precision: 0.9805 - recall: 0.9849\n",
            "Epoch 7: val_loss did not improve from 0.14591\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 55ms/step - accuracy: 0.9827 - loss: 0.0445 - precision: 0.9805 - recall: 0.9849 - val_accuracy: 0.9114 - val_loss: 0.2794 - val_precision: 0.8587 - val_recall: 0.9857 - learning_rate: 1.0000e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9851 - loss: 0.0361 - precision: 0.9832 - recall: 0.9870\n",
            "Epoch 8: val_loss did not improve from 0.14591\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 55ms/step - accuracy: 0.9851 - loss: 0.0361 - precision: 0.9832 - recall: 0.9870 - val_accuracy: 0.9514 - val_loss: 0.1645 - val_precision: 0.9516 - val_recall: 0.9514 - learning_rate: 1.0000e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m2187/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9866 - loss: 0.0315 - precision: 0.9851 - recall: 0.9880\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "\n",
            "Epoch 9: val_loss did not improve from 0.14591\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 54ms/step - accuracy: 0.9866 - loss: 0.0315 - precision: 0.9851 - recall: 0.9880 - val_accuracy: 0.9500 - val_loss: 0.2075 - val_precision: 0.9293 - val_recall: 0.9745 - learning_rate: 1.0000e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9911 - loss: 0.0229 - precision: 0.9891 - recall: 0.9930\n",
            "Epoch 10: val_loss did not improve from 0.14591\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 64ms/step - accuracy: 0.9911 - loss: 0.0229 - precision: 0.9891 - recall: 0.9930 - val_accuracy: 0.9602 - val_loss: 0.1683 - val_precision: 0.9508 - val_recall: 0.9709 - learning_rate: 1.0000e-05\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 56ms/step - accuracy: 0.8588 - loss: 0.4758 - precision: 0.9257 - recall: 0.7761\n",
            "evaluation metrics: [0.46750307083129883, 0.86464923620224, 0.9331133365631104, 0.7834842205047607]\n"
          ]
        }
      ]
    }
  ]
}