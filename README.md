# Deepfake_detector
This Jupyter Notebook provides a comprehensive example of building and training a deepfake detection model using TensorFlow/Keras, emphasizing modularity and best practices for model training. The code is organized into three well-defined classes: DatasetHandler, DeepfakeDetectorModel, and TrainModel, each responsible for a specific aspect of the process.

The DatasetHandler class streamlines dataset management. It automates the download of a zipped dataset from Kaggle using the Kaggle API, handles potential download errors, extracts the dataset, and confirms successful extraction. Leveraging tf.keras.utils.image_dataset_from_directory, it efficiently loads images from the extracted dataset, automatically inferring labels from the directory structure (real/fake) and resizing them to 128x128 pixels. This function also handles shuffling, batching (with a batch size of 64), and splitting the dataset into training, validation, and test sets, ensuring consistent reproducibility with a fixed seed. This automated approach simplifies the data loading process and promotes code reusability for different datasets.

The core of the deepfake detection model lies within the DeepfakeDetectorModel class. It defines a convolutional neural network (CNN) architecture tailored for image classification. The _build_model method constructs a sequential model, starting with an input layer that accepts 128x128 RGB images. A crucial preprocessing step involves rescaling the pixel values to the range [-1, 1] using a rescaling layer, which can improve model stability and performance. The model then employs a series of four convolutional layers with an increasing number of filters (32, 64, 128, and 256), each with a kernel size of 3x3, stride of 1, and 'same' padding to preserve spatial dimensions. These convolutional layers are interspersed with batch normalization layers to accelerate training and mitigate internal covariate shift, and max pooling layers with a 2x2 pool size and stride of 2 to downsample feature maps and introduce translational invariance. The extracted features are then flattened and fed into a fully connected network comprising two dense layers with 512 and 256 units, respectively, both utilizing ReLU activation. Dropout layers with a rate of 0.5 are added after each dense layer to prevent overfitting by randomly deactivating neurons during training. A final dense layer with 128 units precedes the output layer, a single unit with a sigmoid activation function, providing a probability score between 0 and 1, representing the likelihood of an image being a deepfake.

The compile_model method configures the model for training. It uses the Adam optimizer with a configurable learning rate, binary crossentropy loss function suitable for binary classification, and incorporates accuracy, precision, and recall as evaluation metrics, providing a holistic view of model performance. The train_model method employs several callbacks for optimized training: EarlyStopping to halt training if validation loss plateaus, ReduceLROnPlateau to dynamically adjust the learning rate if performance stagnates, and ModelCheckpoint to save the best-performing model based on validation loss. This combination of callbacks ensures efficient training, prevents overfitting, and retains the model with the lowest validation loss. Finally, the evaluate_model method assesses the trained model on the held-out test set, and save_model saves the entire model architecture and weights for later use.

The TrainModel class acts as the central orchestrator, connecting the data handling and model training components. It initializes with dataset-related parameters, facilitating easy configuration. The run_training method downloads, unzips, and loads the dataset, creates an instance of the deepfake detector model, compiles it with a default learning rate of 0.0001, trains the model for a specified number of epochs (defaulting to 10), evaluates the trained model on the test set, and saves the final model. This structured approach enhances code readability, maintainability, and allows for straightforward experimentation with different hyperparameters. The notebook concludes by printing the evaluation metrics and plotting the training history, visualizing the model's learning progress and validating its performance over epochs.
